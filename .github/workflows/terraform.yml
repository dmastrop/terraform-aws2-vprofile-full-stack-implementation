name: "Vprofile IaaC project16 add on"
# Github workflow for terraform IaaC with github actions with project16 full infra with Elasticbeanstalk
# Memcached (ElastiCache), mysql, and RabbitMQ (AmazonMQ).  This is a good example of PaaS beanstalk with several
# SaaS on AWS2

on: 
  push:
    branches:
      - main
      - stage
    # paths:
    #   - terraform/**
    #   # any changes in this folder and if main or stage branch and if push then the workflow will get triggered
  pull_request:
    branches:
      - main
      # when this pull request gets triggered this will trigger the workflow
    # paths:
    #   - terraform/**

# on:
#   # Triggers the workflow on push or pull request events but only for the "main" branch
# This array syntax is not working for 2 branches. Use above list syntax
#   push:
#     branches: [stage main]
#     #paths: [ “terraform/**” ]
#   pull_request:
#     branches: [main]
#     #paths: [ “terraform/**” ]

      

env:
  # Credentials for deployment to AWS with IAM Admin user
  # terraform needs these credentials to connect to AWS
  # secrets.* values have been added to github secrets for this repository.
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  # S3 bucket for the Terraform state
  BUCKET_TF_STATE: ${{ secrets.BUCKET_TF_STATE }}
  AWS_REGION: us-east-1
  #EKS_CLUSTER: project17-eks-vprofile

jobs: 
  terraform:
    # name of the job
    name: "Apply terraform code changes"
    # runner is on github. Use ubuntu-latest for maven, java, and nodejs

    #runs-on: ubuntu-20.04
    runs-on: ubuntu-latest
    # the runner will be on an ubuntu-latest container. Use ubuntu-latest for github runner

    # runs-on: self-hosted
    # this is my self-hosted runner on EC2. Try this out.


    defaults:
      run:
        shell: bash
        #working-directory: ./terraform
        # for project16 the working directory is right at the root of the github repo
        working-directory: ./
        # the working directory will be the terraform folder in the github repo below that is checked out. 
        # The terraform directoy is at the same level as .github, and terraform directory has all the .tf files.
    
    #steps
    steps:
      - name: Checkout the source code. 
      # This is essentially the terraform-project17-iac-vprofile github repository
      # The working dirctory is ./terraform so the steps below for terraform can be executed 
        uses: actions/checkout@v4
        # https://github.com/marketplace/actions/checkout

      - name: Setup Terraform with specified version on the runner.  
      # Runners do not come pre-installed with terraform
        uses: hashicorp/setup-terraform@v2
        # https://github.com/marketplace/actions/hashicorp-setup-terraform
        with:
          #terraform_version: 1.6.3
          #terraform_version: 1.6.6
          terraform_version: 1.5.3
          # comment this out to use the latest version (1.8) of terraform.
          # this configuration and .tf files are not compatible with latest 1.8 so force the 1.6.6
      
      - name: Terraform init
        id: init
        #run: terraform init -upgrade -backend-config="bucket=$BUCKET_TF_STATE"
        run: terraform init -backend-config="bucket=$BUCKET_TF_STATE"
        # BUCKET_TF_STATE is specified in env above.  The state will be stored in the S3 bucket.

      - name: Terraform format
        id: fmt
        run: terraform fmt

        #run: terraform fmt -check
        # remove the check because it is failing on my setup due to comments

        # the format will be checked and will return nonzero error code if the format is incorrect and the workflow will
        # be failed!!  Zero exit code is pass.

      - name: Terraform validate
        id: validate
        run: terraform validate
        # nonzero exit code is a failure and job will fail and exit

      - name: Terraform plan
        id: plan
        run: terraform plan -no-color -input=false -out=planfile
        # no color output, no passing of parameters to other process, store in "planfile"
        # the outfile will help in debugging
        # Saves the plan to a file that can be passed to the Terraform apply command to execute the planned changes. 
        # Any filename is allowed, but the recommended naming convention is to name it “tfplan.” Do not use the “.tf” suffix, 
        # as this will lead Terraform to interpret the file as a configuration script and will fail
        continue-on-error: true
        # this is to pass to next step for the true error checking

      - name: Terraform plan status
        if: steps.plan.outcome == 'failure'
        run: exit 1
        # any nonzero failure code is fine
        # container will be killed and job will fail


     # The following steps will only execute with a push to the main branch (except for Configure AWS credentials, this will be
     # executed with either push to stage or push to main)   

      - name: Terraform Apply (only push and only if push to main)
      # planfile is the output file from the terraform plan step above. We will apply this terraform plan.
        id: apple
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: terraform apply -auto-approve -input=false -parallelism=1 planfile



      # The next block of steps are for aws configure of the runner with the IAM administrator in github secrets
      # Once the aws configure is done, the runner has access to aws cli to apply configurations with kubectl onto
      # the EKS cluster created by the terraform apply, in  previous step.
      # This runs as part of the stage branch. The apply is done only for push to main
      # The runner should have access to AWS credentials as part of the Actions secrets
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}



    # this next block is not required for project16
    #   # Next thing to do is to update the kubeconfig file (~/.kube/config) on the runner so that the runner can execute the 
    #   # kubectl apply of the yaml files to deploy to the k8s cluster (nginx installation, the next step)
    #   # The kubeconfig update requires access to the deployed EKS cluster control (see command below) and IAM Admin credentials (see above)
    #   - name: Get Kube config file from k8s controller/master
    #     id: getconfig
    #     if: steps.apple.outcome == 'success'
    #     # this can only be done if the terraform apply in id:apple is a success and EKS cluster is up and running
    #     run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER }}

    # This block is not required by the project16
    #   ## temporarily comment this out
    #   # Next, Install the nginx ingress controller(frontend) onto the k8s cluster. This can be done through kubectl apply below
    #   # This can only be done if both the terraform apply is successful and the kubeconfig file is updated on the github runner from the k8s controller 
    #   # running in the EKS cluster.  kubectl can only be run if the kubeconfig file is on the runner (see previous step)
    #   - name: Install Ingress Controller
    #     if: steps.apple.outcome == 'success' && steps.getconfig.outcome == 'success'
    #     run: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.3/deploy/static/provider/aws/deploy.yaml



